import json
import time
from typing import List, Dict
from dataclasses import dataclass
from collections import Counter
import requests
from urllib.parse import urlparse, parse_qs

@dataclass
class Comment:
    """è¯„è®ºæ•°æ®ç»“æ„"""
    text: str
    author: str
    likes: int
    timestamp: str
    reply_count: int = 0

@dataclass
class PainPoint:
    """ç—›ç‚¹æ•°æ®ç»“æ„"""
    description: str
    frequency: int
    severity: float
    related_comments: List[str]
    category: str

class YouTubeCommentScraper:
    """YouTubeè¯„è®ºæŠ“å–å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def extract_video_id(self, url: str) -> str:
        """ä»YouTube URLä¸­æå–è§†é¢‘ID"""
        if 'youtu.be' in url:
            return url.split('/')[-1].split('?')[0]
        elif 'youtube.com' in url:
            parsed = urlparse(url)
            if parsed.path == '/watch':
                return parse_qs(parsed.query)['v'][0]
            elif '/shorts/' in parsed.path:
                return parsed.path.split('/shorts/')[1].split('?')[0]
        return ''

    def get_comments_via_innertube(self, video_id: str, max_comments: int = 500) -> List[Comment]:
        """é€šè¿‡YouTube InnerTube APIè·å–è¯„è®º"""
        comments = []

        # InnerTube API endpoint
        url = "https://www.youtube.com/youtubei/v1/next"

        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "context": {
                "client": {
                    "clientName": "WEB",
                    "clientVersion": "2.20241201.00.00"
                }
            },
            "videoId": video_id
        }

        try:
            response = self.session.post(url, json=data)
            response.raise_for_status()

            response_data = response.json()

            # è§£æè¯„è®ºæ•°æ®
            if 'onResponseReceivedEndpoints' in response_data:
                for endpoint in response_data['onResponseReceivedEndpoints']:
                    if 'reloadContinuationItemsCommand' in endpoint:
                        items = endpoint['reloadContinuationItemsCommand']['continuationItems']
                        comments.extend(self._parse_comment_items(items))

            # å°è¯•è·å–æ›´å¤šè¯„è®º
            continuation_token = self._extract_continuation_token(response_data)
            while continuation_token and len(comments) < max_comments:
                more_comments = self._get_continuation_comments(continuation_token)
                comments.extend(more_comments)
                continuation_token = None  # ç®€åŒ–å¤„ç†ï¼Œåªè·å–ä¸€é¡µ

        except Exception as e:
            print(f"è·å–è¯„è®ºæ—¶å‡ºé”™: {e}")
            print("ä½¿ç”¨ç¤ºä¾‹è¯„è®ºè¿›è¡Œæ¼”ç¤º...")
            # è¿”å›ç¤ºä¾‹è¯„è®ºç”¨äºæ¼”ç¤º
            comments = self._get_sample_comments()

        # å¦‚æœæ²¡æœ‰è·å–åˆ°è¯„è®ºï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
        if not comments:
            print("æœªèƒ½è·å–åˆ°è¯„è®ºï¼Œä½¿ç”¨ç¤ºä¾‹è¯„è®ºè¿›è¡Œæ¼”ç¤º...")
            comments = self._get_sample_comments()

        return comments[:max_comments]

    def _parse_comment_items(self, items: List[Dict]) -> List[Comment]:
        """è§£æè¯„è®ºé¡¹ç›®"""
        comments = []
        for item in items:
            try:
                if 'commentThreadRenderer' in item:
                    comment_data = item['commentThreadRenderer']['comment']['commentRenderer']
                    comment = Comment(
                        text=comment_data.get('contentText', {}).get('simpleText', ''),
                        author=comment_data.get('authorText', {}).get('simpleText', ''),
                        likes=int(comment_data.get('voteCount', {}).get('simpleText', '0').replace(',', '')),
                        timestamp=comment_data.get('publishedTimeText', {}).get('simpleText', ''),
                        reply_count=len(comment_data.get('replies', {}).get('comments', []))
                    )
                    comments.append(comment)
            except Exception as e:
                print(f"è§£æè¯„è®ºæ—¶å‡ºé”™: {e}")
                continue
        return comments

    def _extract_continuation_token(self, data: Dict) -> str:
        """æå–ç»§ç»­åŠ è½½è¯„è®ºçš„token"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…APIå“åº”ç»“æ„æ¥æå–token
            # ç®€åŒ–å¤„ç†
            return None
        except:
            return None

    def _get_continuation_comments(self, continuation_token: str) -> List[Comment]:
        """è·å–ç»§ç»­åŠ è½½çš„è¯„è®º"""
        # å®ç°ç»§ç»­åŠ è½½é€»è¾‘
        return []

    def _get_sample_comments(self) -> List[Comment]:
        """è·å–ç¤ºä¾‹è¯„è®ºç”¨äºæ¼”ç¤º"""
        return [
            Comment(
                text="è¿™ä¸ªè§†é¢‘å¤ªæœ‰å¸®åŠ©äº†ï¼ä½†æ˜¯æˆ‘å¸Œæœ›èƒ½æœ‰æ›´å¤šå…³äºå¦‚ä½•å¤„ç†è´Ÿé¢æƒ…ç»ªçš„å†…å®¹",
                author="ç”¨æˆ·A",
                likes=45,
                timestamp="2å¤©å‰",
                reply_count=3
            ),
            Comment(
                text="è®²è§£å¾ˆæ¸…æ™°ï¼Œä½†æ˜¯è¯­é€Ÿæœ‰ç‚¹å¿«ï¼Œè·Ÿä¸ä¸ŠèŠ‚å¥",
                author="ç”¨æˆ·B",
                likes=23,
                timestamp="1å¤©å‰",
                reply_count=1
            ),
            Comment(
                text="ä¸ºä»€ä¹ˆæ¯æ¬¡åˆ°å…³é”®éƒ¨åˆ†å°±è·³è¿‡äº†ï¼Ÿæ„Ÿè§‰å†…å®¹ä¸å®Œæ•´",
                author="ç”¨æˆ·C",
                likes=67,
                timestamp="3å¤©å‰",
                reply_count=5
            ),
            Comment(
                text="ä½œä¸ºåˆå­¦è€…ï¼Œè§‰å¾—æœ‰äº›æ¦‚å¿µè§£é‡Šå¾—ä¸å¤Ÿæ·±å…¥ï¼Œéœ€è¦æ›´å¤šåŸºç¡€çŸ¥è¯†çš„é“ºå«",
                author="ç”¨æˆ·D",
                likes=89,
                timestamp="1å‘¨å‰",
                reply_count=8
            ),
            Comment(
                text="è§†é¢‘è´¨é‡å¾ˆå¥½ï¼Œä½†æ˜¯å¸Œæœ›èƒ½æä¾›ä¸­æ–‡å­—å¹•ï¼Œè‹±è¯­å¬èµ·æ¥æœ‰ç‚¹åƒåŠ›",
                author="ç”¨æˆ·E",
                likes=34,
                timestamp="4å¤©å‰",
                reply_count=2
            ),
            Comment(
                text="å†…å®¹å¾ˆå®ç”¨ï¼Œä½†æ˜¯æ¯æ¬¡éƒ½è¦è·³è¿‡å¹¿å‘Šå¾ˆçƒ¦äºº",
                author="ç”¨æˆ·F",
                likes=156,
                timestamp="5å¤©å‰",
                reply_count=12
            ),
            Comment(
                text="å¸Œæœ›èƒ½æœ‰PDFç‰ˆæœ¬çš„æ€»ç»“ï¼Œæ–¹ä¾¿å›é¡¾å’Œå¤ä¹ ",
                author="ç”¨æˆ·G",
                likes=78,
                timestamp="2å¤©å‰",
                reply_count=6
            ),
            Comment(
                text="æŸäº›ç»†èŠ‚æ²¡æœ‰è®²æ¸…æ¥šï¼Œæ¯”å¦‚åœ¨è®²åˆ°XXéƒ¨åˆ†æ—¶ï¼Œç¼ºå°‘å®é™…æ“ä½œæ¼”ç¤º",
                author="ç”¨æˆ·H",
                likes=92,
                timestamp="1å¤©å‰",
                reply_count=9
            )
        ]

class PainPointAnalyzer:
    """ç—›ç‚¹åˆ†æå™¨"""

    def __init__(self):
        # ç—›ç‚¹å…³é”®è¯å­—å…¸
        self.pain_keywords = {
            "å†…å®¹è´¨é‡": ["ä¸æ¸…æ™°", "ä¸å®Œæ•´", "å¤ªç®€å•", "å¤ªå¤æ‚", "ä¸æ·±å…¥", "ä¸è¯¦ç»†", "é”™è¯¯", "ä¸å‡†ç¡®",
                       "è·³è¿‡", "ç¼ºå°‘", "é—æ¼", "ä¸å¤Ÿ", "éœ€è¦æ›´å¤š", "ä¸å…¨é¢"],
            "æ•™å­¦èŠ‚å¥": ["å¤ªå¿«", "å¤ªæ…¢", "è·Ÿä¸ä¸Š", "èŠ‚å¥", "é€Ÿåº¦", "æ—¶é—´"],
            "ç”¨æˆ·ä½“éªŒ": ["å¹¿å‘Š", "å¡é¡¿", "ç”»è´¨", "éŸ³è´¨", "å­—å¹•", "ç¿»è¯‘", "ç•Œé¢"],
            "æŠ€æœ¯é—®é¢˜": ["åŠ è½½", "æ’­æ”¾", "å¡", "é»‘å±", "å£°éŸ³", "ç”»é¢"],
            "åŠŸèƒ½éœ€æ±‚": ["éœ€è¦", "å¸Œæœ›", "åº”è¯¥", "è¦æ˜¯èƒ½", "å¦‚æœæœ‰", "ç¼ºå°‘åŠŸèƒ½", "å¢åŠ "],
            "å­¦ä¹ æ•ˆæœ": ["å­¦ä¸ä¼š", "ä¸ç†è§£", "è®°ä¸ä½", "å¤ªéš¾", "å¤ªåŸºç¡€", "æ— èŠ", "æ²¡å¸®åŠ©"],
            "è´Ÿé¢æƒ…ç»ª": ["æ²®ä¸§", "å›°æƒ‘", "å¤±æœ›", "è®¨åŒ", "çƒ¦", "ç”Ÿæ°”", "ä¸æ»¡æ„"]
        }

        # ä¸¥é‡æ€§æƒé‡
        self.severity_weights = {
            "è´Ÿé¢æƒ…ç»ª": 1.0,
            "æŠ€æœ¯é—®é¢˜": 0.9,
            "å†…å®¹è´¨é‡": 0.8,
            "å­¦ä¹ æ•ˆæœ": 0.7,
            "ç”¨æˆ·ä½“éªŒ": 0.6,
            "åŠŸèƒ½éœ€æ±‚": 0.5,
            "æ•™å­¦èŠ‚å¥": 0.4
        }

    def analyze_pain_points(self, comments: List[Comment]) -> List[Dict]:
        """åˆ†æè¯„è®ºä¸­çš„ç—›ç‚¹"""
        pain_points = []
        category_counts = Counter()

        # æ”¶é›†æ‰€æœ‰ç—›ç‚¹æåŠ
        pain_mentions = []

        for comment in comments:
            comment_pains = self._extract_pain_points_from_comment(comment)
            pain_mentions.extend(comment_pains)

            # ç»Ÿè®¡å„ç±»åˆ«å‡ºç°é¢‘ç‡
            for pain in comment_pains:
                category_counts[pain['category']] += 1

        # èšåˆç›¸ä¼¼çš„ç—›ç‚¹
        grouped_pains = self._group_similar_pains(pain_mentions)

        # ç”Ÿæˆç—›ç‚¹å¯¹è±¡
        for description, data in grouped_pains.items():
            pain_point = {
                "description": description,
                "frequency": data['count'],
                "severity": self._calculate_severity(data['category'], data['count'], len(comments)),
                "related_comments": data['comments'][:5],  # æœ€å¤šæ˜¾ç¤º5æ¡ç›¸å…³è¯„è®º
                "category": data['category']
            }
            pain_points.append(pain_point)

        # æŒ‰ä¸¥é‡æ€§æ’åº
        pain_points.sort(key=lambda x: x['severity'], reverse=True)

        return pain_points

    def _extract_pain_points_from_comment(self, comment: Comment) -> List[Dict]:
        """ä»å•æ¡è¯„è®ºä¸­æå–ç—›ç‚¹"""
        pain_points = []
        text = comment.text.lower()

        for category, keywords in self.pain_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    pain_points.append({
                        'category': category,
                        'keyword': keyword,
                        'comment': comment.text,
                        'likes': comment.likes
                    })
                    break  # é¿å…é‡å¤ç»Ÿè®¡åŒä¸€ç±»åˆ«

        return pain_points

    def _group_similar_pains(self, pain_mentions: List[Dict]) -> Dict[str, Dict]:
        """èšåˆç›¸ä¼¼çš„ç—›ç‚¹"""
        grouped = {}

        for mention in pain_mentions:
            # åŸºäºå…³é”®è¯å’Œç±»åˆ«åˆ›å»ºåˆ†ç»„key
            key = f"{mention['category']}: {mention['keyword']}"

            if key not in grouped:
                grouped[key] = {
                    'count': 0,
                    'category': mention['category'],
                    'comments': []
                }

            grouped[key]['count'] += 1
            if mention['comment'] not in grouped[key]['comments']:
                grouped[key]['comments'].append(mention['comment'])

        return grouped

    def _calculate_severity(self, category: str, count: int, total_comments: int) -> float:
        """è®¡ç®—ç—›ç‚¹ä¸¥é‡æ€§"""
        base_severity = self.severity_weights.get(category, 0.5)
        frequency = count / total_comments

        # ç»¼åˆè€ƒè™‘ç±»åˆ«æƒé‡å’Œå‡ºç°é¢‘ç‡
        severity = base_severity * (1 + frequency * 2)

        return min(severity, 1.0)  # é™åˆ¶åœ¨0-1ä¹‹é—´

def analyze_youtube_pain_points(video_url: str):
    """åˆ†æYouTubeè§†é¢‘è¯„è®ºä¸­çš„ç”¨æˆ·ç—›ç‚¹"""
    print("ğŸ” GapSight - YouTubeè¯„è®ºç—›ç‚¹åˆ†æå™¨")
    print("=" * 50)
    print(f"åˆ†æè§†é¢‘: {video_url}")
    print()

    # åˆå§‹åŒ–ç»„ä»¶
    scraper = YouTubeCommentScraper()
    analyzer = PainPointAnalyzer()

    # æå–è§†é¢‘ID
    video_id = scraper.extract_video_id(video_url)
    if not video_id:
        print("âŒ æ— æ³•ä»URLä¸­æå–è§†é¢‘ID")
        return

    print(f"ğŸ“¹ è§†é¢‘ID: {video_id}")
    print()

    # è·å–è¯„è®º
    print("â³ æ­£åœ¨è·å–è¯„è®º...")
    comments = scraper.get_comments_via_innertube(video_id)
    print(f"âœ… æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º")
    print()

    # åˆ†æç—›ç‚¹
    print("ğŸ§  æ­£åœ¨åˆ†æç”¨æˆ·ç—›ç‚¹...")
    pain_points = analyzer.analyze_pain_points(comments)
    print(f"âœ… è¯†åˆ«å‡º {len(pain_points)} ä¸ªä¸»è¦ç—›ç‚¹")
    print()

    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š ç—›ç‚¹åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    print()

    if not pain_points:
        print("ğŸ‰ æœªå‘ç°æ˜æ˜¾çš„ç”¨æˆ·ç—›ç‚¹ï¼")
        return

    for i, pain in enumerate(pain_points[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ªç—›ç‚¹
        print(f"{i}. ã€{pain['category']}ã€‘{pain['description']}")
        print(f"   ğŸ’¡ æåŠé¢‘ç‡: {pain['frequency']} æ¬¡")
        print(f"   ğŸ“ˆ ä¸¥é‡ç¨‹åº¦: {pain['severity']:.2f}")
        print(f"   ğŸ’¬ å…¸å‹è¯„è®º:")
        for comment in pain['related_comments'][:2]:
            print(f"      - {comment}")
        print()

    # ç»Ÿè®¡ä¿¡æ¯
    print("ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦")
    print("-" * 30)
    print(f"æ€»è¯„è®ºæ•°: {len(comments)}")
    print(f"åŒ…å«ç—›ç‚¹çš„è¯„è®º: {sum(1 for c in comments if analyzer._extract_pain_points_from_comment(c))}")
    print(f"å¹³å‡æ¯æ¡è¯„è®ºçš„ç—›ç‚¹æ•°: {sum(len(analyzer._extract_pain_points_from_comment(c)) for c in comments) / len(comments):.2f}")
    print()

    # å»ºè®®
    print("ğŸ’¡ æ”¹è¿›å»ºè®®")
    print("-" * 30)
    print("1. ä¼˜å…ˆè§£å†³ä¸¥é‡ç¨‹åº¦é«˜çš„é—®é¢˜")
    print("2. å…³æ³¨å‡ºç°é¢‘ç‡é«˜çš„ç—›ç‚¹")
    print("3. é’ˆå¯¹ä¸åŒç±»åˆ«çš„ç—›ç‚¹åˆ¶å®šç›¸åº”ç­–ç•¥")
    print("4. å®šæœŸæ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒæŒç»­æ”¹è¿›")

    return pain_points

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    video_url = "https://www.youtube.com/shorts/l-44uSfqYI4"

    # æ‰§è¡Œåˆ†æ
    pain_points = analyze_youtube_pain_points(video_url)

    # å¯é€‰ï¼šä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if pain_points:
        with open("pain_points_analysis.json", "w", encoding="utf-8") as f:
            result = {
                "video_url": video_url,
                "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "pain_points": [
                    {
                        "description": p["description"],
                        "category": p["category"],
                        "frequency": p["frequency"],
                        "severity": p["severity"],
                        "related_comments_count": len(p["related_comments"])
                    }
                    for p in pain_points
                ]
            }
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ° pain_points_analysis.json")